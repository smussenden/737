---
title: "Homework 3 INST737 Sean Mussenden"
output: html_notebook
---

This code is designed to work through homework 3, detailed here. https://docs.google.com/document/d/15pwjdBmrQvQiQV320GrPijokb0kDjutqtFDQKBCdyzk/edit.  

We are building a classifier to predict whether an answer to a quiz show question is correct, based on information about the question and the guess. The data is here: https://www.kaggle.com/c/digging-into-data-hw3-question-answering2/data

Let's load the needed libraries and set up our environment.

```{r, include=FALSE}
# Setting up the environment
# Remove all previous objects
rm(list=ls(all=TRUE))

# Load the tidyverse
library(tidyverse)

# Load e1071 for the SVM (Support Vector Machine) functions
library(e1071)

# Load the library nnet, which will allow us to run multinom, which is a form of logistic regression different than glm. 
library(nnet)

# Load cwhmisc and two dependencies, lattice and grid, though I'm not sure what these are for
library(cwhmisc) # not sure what these next three are for
library(lattice)
library(grid)
library(rpart)

# This is for testing the accuracy of our logistic regression, if we decide not to use the classmatch function provided by prof. sayed
library(pscl) 
```

Question 2: (30 points) Build the best classifier you can with the given data, documenting the choices that you make.  Try using logistic regression, SVM (multiple kernels), and decision trees.  Create a table with your accuracy with each of these methods. Look at where youâ€™re making mistakes.  Can you see any patterns?
 
```{r, echo=TRUE}
# Define a function called "record_performance" that accepts five variable arguments model_type, df, name, model and test.  We'll use this to measure the performance of our models we're building -- svm, decision tree, logisitc regression, naive bayes. Note that the if-else function allows us to change the "type" for prediction so that the prediction works.   

record_performance <- function( model_type, df, name, model, test) {
  # if-else statement to change pred variable depending on model type 
  if (model_type=="svm"){
    pred <- predict(model, test)
  }
  else if (model_type=="tree"){
    pred <- predict(model, test,type="class")
  }
  else if (model_type=="logit"){
    pred <- predict(model, test,type="class") #we may need to store this as type="response"
  }
  # create a table with the prediction from the model and the actual value in the test data
  table <- table(pred = pred, true=test$corr)
  # calculate the "score" the degree to which the values in our prediction agree (classAgreement) with values in the test data.  Then add them to a table we create with multiple model predictions.
  df <- rbind(df, data.frame(model=c(name), score=c(classAgreement(table)$diag)))
  return(df)
}

# Here are examples of how I'll test SVM, LOGISTIC REGRESSION, DECISION TREE
# results <- record_performance("svm", results, "body_score", svm(corr ~ body_score, data=trainset), testset)
# results <- record_performance('logit', results , "body_score", glm(corr ~ body_score, data=trainset, family=binomial), testset) # May need to change this to say multinom instead of glm to work
# results <- record_performance("tree", results , "body_score", rpart(corr ~ body_score, data=trainset,method="class"), testset)
```

Let's load in our datasets

```{r echo=FALSE}
# Load in the data see record-layout.txt for details https://www.kaggle.com/c/digging-into-data-hw3-question-answering2/data

# Train is the dataset we'll work with to build and perfect our models
train <- read_csv("csv/qb.train.csv", col_names= TRUE)

# Test is the dataset that we'll test our answers against.  It doesn't contain a corr column, which tells us if our answers are correct.  Those are contained in "guess", so we could do some matching to check.
test <- read_csv("csv/qb.test.csv", col_names= TRUE)
guess <- read_csv("csv/qb.guess.csv", col_names= TRUE)

# Create a copy of the training data called full, which we'll use to do feature engineering -- transforming some of the columns to use to build a better model.   
full <- train

```

Let's do some feature engineering, create some new columns we can use to build our models.

```{r}
# First, create a column that counts the number of characters in the questions. Longer questions are those that have more information in them - people waited longer before buzzing in.
full$obs_len <- apply(full, 1, function(x) {nchar(x['text'])})

# Creates a column that standardizes the values in obs_len along a normalized scale (I think it's SD from the mean)
full$scale_len <- scale(full$obs_len)

# Creates a column that standardizes the values in body_score along a normalized scale (I think it's SD from the mean)
full$scale_score <- scale(full$body_score)

# Creates a column that checks whether the page name has something in parens that also appears in the body of the question text.
paren_match <- function(page, text) {
  start <- cpos(page, "(")
  end <- cpos(page, ")")
  if (!is.na(start) && !is.na(end)) {
    search <- substring(page, start + 1, end - 1)
    return(grepl(tolower(search), tolower(text), fixed=TRUE))
  } else {
    return(FALSE)
  }
}
full$paren_match <- apply(full, 1, function(x) {paren_match(x['page'], x['text'])})

# Normalizes the inlinks into a page by standardizes the values in inlinks along a normalized scale (I think it's SD from the mean)
full$log_links <- scale(log(as.numeric(full$inlinks) + 1))

```
Lastly, let's convert the variable we're trying to predict -- whether question is correct -- to 1 and 0 so we can use them in the analysis. 
```{r}
full$corr[full$corr == "False"] <- 0
full$corr[full$corr == "True"] <- 1

```
And let's split our "full" data set into a training slice and a testing slice (I know, even though we have a separate test set). 1/5 in testset and 4/5 in trainset.

```{r}
index <- 1:nrow(full)
testindex <- sample(index, trunc(length(index)/5))
testset <- full[testindex,]
trainset <- full[-testindex,]
```

Let's build different Logistic Regression models, and test their accuracy.

```{r}
# LOGISTIC REGRESSION 

# Get the most frequent baseline, which divides number of false answers by number of total observations to use as a base measurement. This is how well humans did on guessing answers to questions. We need to do better than this with our predictive model. So any model we build needs to do a better job than the humans at getting the right answer. 
lr.mfc_baseline <- sum(testset$corr == 0) / nrow(testset)
lr.results <- data.frame(model=c("MFC"), score=c(lr.mfc_baseline))

# Now let's add single variable logsitic regression models to the list. 
lr.results <- record_performance("logit", lr.results, "body_score", multinom(corr ~ body_score, data=trainset),testset)
lr.results <- record_performance("logit", lr.results, "tournaments", multinom(corr ~ tournaments, data=trainset),testset)
lr.results <- record_performance("logit", lr.results, "answer_type", multinom(corr ~ answer_type, data=trainset),testset)
lr.results <- record_performance("logit", lr.results, "links", multinom(corr ~ inlinks, data=trainset),testset)
lr.results <- record_performance("logit", lr.results, "obs_len", multinom(corr ~ obs_len, data=trainset),testset)
lr.results <- record_performance("logit", lr.results, "scale_len", multinom(corr ~ scale_len, data=trainset),testset)
lr.results <- record_performance("logit", lr.results, "scale_score", multinom(corr ~ scale_score, data=trainset),testset)
lr.results <- record_performance("logit", lr.results, "paren_match", multinom(corr ~ paren_match, data=trainset),testset)
lr.results <- record_performance("logit", lr.results, "log_links", multinom(corr ~ log_links, data=trainset),testset)

# Now let's do multi logistic models and see what we get.
lr.results <- record_performance("logit", lr.results, "score+len", multinom(corr ~ obs_len + body_score, data=trainset),testset)
lr.results <- record_performance("logit", lr.results, "paren+len", multinom(corr ~ obs_len + paren_match, data=trainset),testset)
lr.results <- record_performance("logit", lr.results, "score+paren_match", multinom(corr ~ scale_score + paren_match, data=trainset),testset)
lr.results <- record_performance("logit", lr.results, "score+len+paren_match", multinom(corr ~ scale_len + scale_score + paren_match, data=trainset),testset)
lr.results <- record_performance("logit", lr.results, "score+len+links+paren_match", multinom(corr ~ scale_len + scale_score + log_links + paren_match, data=trainset),testset)
lr.results <- record_performance("logit", lr.results, "score+links+paren_match+tournaments+answertype", multinom(corr ~ scale_len + scale_score + paren_match + tournaments + answer_type, data=trainset),testset)
lr.results
```
Our highest performing logistic regression model, which takes into account body score, question length and the parentheses match field, correctly predicting 72.4 percent of questions. 

Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Cmd+Option+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Cmd+Shift+K* to preview the HTML file).
